/// Copyright (c) 2019 Razeware LLC
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in
/// all copies or substantial portions of the Software.
///
/// Notwithstanding the foregoing, you may not use, copy, modify, merge, publish,
/// distribute, sublicense, create a derivative work, and/or sell copies of the
/// Software in any work that is designed, intended, or marketed for pedagogical or
/// instructional purposes related to programming, coding, application development,
/// or information technology.  Permission for such use, copying, modification,
/// merger, publication, distribution, sublicensing, creation of derivative works,
/// or sale is expressly withheld.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
/// THE SOFTWARE.

import AVFoundation
import UIKit
import Vision

class FaceDetectionViewController: UIViewController {
  @IBOutlet var faceView: FaceView!
  @IBOutlet var laserView: LaserView!
  @IBOutlet var faceLaserLabel: UILabel!
	
	var sequenceHandler = VNSequenceRequestHandler()
  
  let session = AVCaptureSession()
  var previewLayer: AVCaptureVideoPreviewLayer!
  
  let dataOutputQueue = DispatchQueue(
    label: "video data queue",
    qos: .userInitiated,
    attributes: [],
    autoreleaseFrequency: .workItem
	)

  var faceViewHidden = false
  
  var maxX: CGFloat = 0.0
  var midY: CGFloat = 0.0
  var maxY: CGFloat = 0.0

  override func viewDidLoad() {
    super.viewDidLoad()
    configureCaptureSession()
    
    laserView.isHidden = true
    
    maxX = view.bounds.maxX
    midY = view.bounds.midY
    maxY = view.bounds.maxY
    
    session.startRunning()
  }
}

// MARK: - Gesture methods

extension FaceDetectionViewController {
  @IBAction func handleTap(_ sender: UITapGestureRecognizer) {
    faceView.isHidden.toggle()
    laserView.isHidden.toggle()
    faceViewHidden = faceView.isHidden
    
    if faceViewHidden {
      faceLaserLabel.text = "Lasers"
    } else {
      faceLaserLabel.text = "Face"
    }
  }
}

// MARK: - Video Processing methods

extension FaceDetectionViewController {
  func configureCaptureSession() {
    // Define the capture device we want to use
		guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera,
                                               for: .video,
                                               position: .front) else {
      fatalError("No front video camera available")
    }
    
    // Connect the camera to the capture session input
    do {
      let cameraInput = try AVCaptureDeviceInput(device: camera)
      session.addInput(cameraInput)
    } catch {
      fatalError(error.localizedDescription)
    }
    
    // Create the video data output
    let videoOutput = AVCaptureVideoDataOutput()
    videoOutput.setSampleBufferDelegate(self, queue: dataOutputQueue)
    videoOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA]
    
    // Add the video output to the capture session
    session.addOutput(videoOutput)
    
    let videoConnection = videoOutput.connection(with: .video)
    videoConnection?.videoOrientation = .portrait
    
    // Configure the preview layer
    previewLayer = AVCaptureVideoPreviewLayer(session: session)
    previewLayer.videoGravity = .resizeAspectFill
    previewLayer.frame = view.bounds
    view.layer.insertSublayer(previewLayer, at: 0)
  }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate methods

extension FaceDetectionViewController: AVCaptureVideoDataOutputSampleBufferDelegate {
  func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
		guard let imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
		
		// let detectFaceRequest = VNDetectFaceRectanglesRequest(completionHandler: detectedFace)
		let detectFaceRequest = VNDetectFaceLandmarksRequest(completionHandler: detectedFace)
		
		do {
			try sequenceHandler.perform([detectFaceRequest], on: imageBuffer, orientation: .leftMirrored)
		} catch {
			print(error.localizedDescription)
		}
  }
	
	func detectedFace(request: VNRequest, error: Error?) {
		guard let results = request.results as? [VNFaceObservation],
					let result = results.first else {
			faceView.clear()
			return
		}
		
		// let box = result.boundingBox
		// faceView.boundingBox = convert(rect: box)
		
		// DispatchQueue.main.async {
			// self.faceView.setNeedsDisplay()
		// }
		
		updateFaceView(for: result)
	}
	
	func updateFaceView(for result: VNFaceObservation) {
		defer { DispatchQueue.main.async { self.faceView.setNeedsDisplay() } }
		
		let box = result.boundingBox
		faceView.boundingBox = convert(rect: box)
		
		guard let landmarks = result.landmarks else { return }
		
		if let leftEye = landmark(points: landmarks.leftEye?.normalizedPoints, to: result.boundingBox) {
			faceView.leftEye = leftEye
		}
		
		if let rightEye = landmark(
			points: landmarks.rightEye?.normalizedPoints,
			to: result.boundingBox) {
			faceView.rightEye = rightEye
		}
				
		if let leftEyebrow = landmark(
			points: landmarks.leftEyebrow?.normalizedPoints,
			to: result.boundingBox) {
			faceView.leftEyebrow = leftEyebrow
		}
				
		if let rightEyebrow = landmark(
			points: landmarks.rightEyebrow?.normalizedPoints,
			to: result.boundingBox) {
			faceView.rightEyebrow = rightEyebrow
		}
				
		if let nose = landmark(
			points: landmarks.nose?.normalizedPoints,
			to: result.boundingBox) {
			faceView.nose = nose
		}
				
		if let outerLips = landmark(
			points: landmarks.outerLips?.normalizedPoints,
			to: result.boundingBox) {
			faceView.outerLips = outerLips
		}
				
		if let innerLips = landmark(
			points: landmarks.innerLips?.normalizedPoints,
			to: result.boundingBox) {
			faceView.innerLips = innerLips
		}
				
		if let faceContour = landmark(
			points: landmarks.faceContour?.normalizedPoints,
			to: result.boundingBox) {
			faceView.faceContour = faceContour
		}

	}
	
	func landmark(point: CGPoint, to rect: CGRect) -> CGPoint {
		let absolute = point.absolutePoint(in: rect)
		let converted = previewLayer.layerPointConverted(fromCaptureDevicePoint: absolute)
		return converted
	}
	
	func landmark(points: [CGPoint]?, to rect: CGRect) -> [CGPoint]? {
		return points?.compactMap { landmark(point: $0, to: rect) }
	}
	
	func convert(rect: CGRect) -> CGRect {
		let origin = previewLayer.layerPointConverted(fromCaptureDevicePoint: rect.origin)
		let size = previewLayer.layerPointConverted(fromCaptureDevicePoint: rect.size.cgPoint)
		return CGRect(origin: origin, size: size.cgSize)
	}
}
